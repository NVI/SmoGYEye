{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, clear outputs (Cell > All Output > Clear) before committing to Git\n",
    "# There might be a better way\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree, preprocessing\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read wildfire data from SQLITE database\n",
    "cnx = sqlite3.connect('FPA_FOD_20170508.sqlite')\n",
    "df = pd.read_sql_query(\"SELECT FIRE_YEAR,DISCOVERY_TIME,STAT_CAUSE_DESCR,CONT_DATE,CONT_TIME,LATITUDE,LONGITUDE,STATE,DISCOVERY_DATE,FIRE_SIZE,FIRE_SIZE_CLASS FROM 'Fires'\", cnx)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fire discovery and containment dates to yyyy-mm-dd\n",
    "df['DISCOVERY_DATE'] = pd.to_datetime(df['DISCOVERY_DATE'] - pd.Timestamp(0).to_julian_date(), unit='D')\n",
    "df['CONT_DATE'] = pd.to_datetime(df['CONT_DATE'] - pd.Timestamp(0).to_julian_date(), unit='D')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns for month, date, and weekday for discovery and containment dates\n",
    "\n",
    "df['DISCOVERY_MONTH'] = pd.DatetimeIndex(df['DISCOVERY_DATE']).month\n",
    "df['DISCOVERY_DAY'] = pd.DatetimeIndex(df['DISCOVERY_DATE']).day\n",
    "df['DISCOVERY_DAY_OF_WEEK'] = df['DISCOVERY_DATE'].dt.weekday_name\n",
    "\n",
    "#df['CONT_DATE'].fillna(0)\n",
    "df['CONT_MONTH'] = pd.DatetimeIndex(df['CONT_DATE']).month\n",
    "df['CONT_DAY'] = pd.DatetimeIndex(df['CONT_DATE']).day\n",
    "df['CONT_DAY_OF_WEEK'] = df['CONT_DATE'].dt.weekday_name\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "df['STAT_CAUSE_DESCR'] = le.fit_transform(df['STAT_CAUSE_DESCR'])\n",
    "df['STATE'] = le.fit_transform(df['STATE'])\n",
    "df['DISCOVERY_DAY_OF_WEEK'] = le.fit_transform(df['DISCOVERY_DAY_OF_WEEK'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['CONT_DAY_OF_WEEK']=df['CONT_DAY_OF_WEEK'].fillna(\"Unknown\")\n",
    "df['CONT_DAY']=df['CONT_DAY'].fillna(\"0\")\n",
    "df['CONT_MONTH']=df['CONT_MONTH'].fillna(\"0\")\n",
    "df['CONT_TIME']=df['CONT_TIME'].fillna(\"0\")\n",
    "df['DISCOVERY_TIME']=df['DISCOVERY_TIME'].fillna(\"0\")\n",
    "\n",
    "df['CONT_DAY_OF_WEEK'] = le.fit_transform(df['CONT_DAY_OF_WEEK'])\n",
    "df['FIRE_SIZE_CLASS'] = le.fit_transform(df['FIRE_SIZE_CLASS'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CONT_MONTH']=df['CONT_MONTH'].astype('Float64')\n",
    "df['CONT_DAY']=df['CONT_DAY'].astype('Float64')\n",
    "#df['CONT_TIME']=df['CONT_TIME'].astype('Float64')\n",
    "#df['DISCOVERY_DATE']=df['DISCOVERY_DATE'].astype('Float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['CONT_DATE','CONT_TIME','DISCOVERY_TIME','STATE','FIRE_SIZE'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df:\n",
    "    print(item)\n",
    "    print(df[item].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=df['FIRE_SIZE_CLASS']\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits=df.drop(['FIRE_SIZE','FIRE_SIZE_CLASS','DISCOVERY_DATE','CONT_DATE','STATE','CONT_TIME','DISCOVERY_TIME'],axis=1)\n",
    "logits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(logits,labels,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg.predict(x_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cat=keras.utils.to_categorical(labels,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(logits,labels_cat,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(BatchNormalization(input_shape=[10]))\n",
    "model.add(Dense(500,kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(300,kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(7,kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.01, momentum=0.9,decay=1e-6),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,epochs=10,validation_data=(np.array(x_test),np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main focus:\n",
    "Write the algorithm, who joins the wildfire data with the airquality data over the closest latitude and longitude and the closest date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_site_list(site_file):\n",
    "    # Make sure that state, county, and site codes are read as strings\n",
    "    # instead of numbers\n",
    "    col_types = {'State Code': str, 'County Code': str, 'Site Number': str}\n",
    "\n",
    "    sites = pd.read_csv(site_file, dtype=col_types)\n",
    "\n",
    "    # Columns that can be dropped from the site listing\n",
    "    cols_to_drop = [\n",
    "        'Land Use', 'Location Setting', 'Met Site State Code',\n",
    "        'Met Site County Code', 'Met Site Site Number', 'Met Site Type',\n",
    "        'Met Site Distance', 'Met Site Direction', 'GMT Offset',\n",
    "        'Owning Agency', 'Local Site Name', \"Address\", \"Zip Code\",\n",
    "        \"State Name\", \"County Name\", \"City Name\", \"CBSA Name\", \"Tribe Name\",\n",
    "        \"Extraction Date\"\n",
    "    ]\n",
    "\n",
    "    sites = sites.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Convert site closing dates to datetime64 objects (does not work\n",
    "    # with read_csv())\n",
    "    sites['Site Closed Date'] = pd.to_datetime(sites['Site Closed Date'])\n",
    "\n",
    "    # Get sites that have been closed down before 1992 and drop them\n",
    "    # from the list\n",
    "    outdated = sites.loc[\n",
    "        sites['Site Closed Date'] < pd.to_datetime('01-01-1992')]\n",
    "    sites = sites.drop(outdated.index)\n",
    "\n",
    "    # Get the complete site code, add it as a column, and finally\n",
    "    # use the site code as the index\n",
    "    sites['site-code'] = sites['State Code'] + '-' + sites[\n",
    "        'County Code'] + '-' + sites['Site Number']\n",
    "    sites = sites.set_index('site-code')\n",
    "\n",
    "    # Divide the data into two Series: one with latitude coordinates and one\n",
    "    # with longitude coordinates\n",
    "    sites_lat = sites['Latitude']\n",
    "    sites_lon = sites['Longitude']\n",
    "\n",
    "    # Transform the site listings into a dictionary where keys are site\n",
    "    # codes and values are coordinates}\n",
    "    sites_lat = sites_lat.to_dict()\n",
    "    sites_lon = sites_lon.to_dict()\n",
    "\n",
    "    # Return the two dictionaries with the coordinates\n",
    "    return sites_lat, sites_lon\n",
    "\n",
    "\n",
    "def preprocess_aqi(sites_lat, sites_lon, aqi_file):\n",
    "    # Read daily AQI measurements from year XXXX\n",
    "    \n",
    "    # Specify used column names and types to make CSV parsing more efficient\n",
    "    col_types = {'AQI': np.int32, 'Category': str, 'Defining Parameter': str,\n",
    "                 'Defining Site': str}\n",
    "    \n",
    "    col_names = list(col_types.keys()) + ['Date']\n",
    "    \n",
    "    aqi = pd.read_csv(aqi_file, usecols=col_names, dtype=col_types, parse_dates=['Date'])\n",
    "\n",
    "    # Add column containing the site information (including location\n",
    "    # coordinates) to each AQI measurement\n",
    "    aqi['Latitude'] = aqi['Defining Site'].map(sites_lat)\n",
    "    aqi['Longitude'] = aqi['Defining Site'].map(sites_lon)\n",
    "\n",
    "    return aqi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionaries containing latitude and longitude coordinates for AQI measurement sites\n",
    "sites_lat, sites_lon = preprocess_site_list('data/aqi/aqs_sites.csv')\n",
    "\n",
    "# Generate a list of years \n",
    "years = [str(y) for y in range(1992, 2016)]\n",
    "\n",
    "# Generate a dictionary where keys are years (as strings) and values are DataFrames\n",
    "# containing the AQI data with coordinates added\n",
    "aqi = {y: preprocess_aqi(sites_lat, sites_lon, 'data/aqi/daily_aqi_by_county_{}.csv'.format(y)) for y in years}\n",
    "\n",
    "# Combine all AQI data into a single large DataFrame (~7 million rows)\n",
    "aqi_all = pd.concat([aqi[y] for y in aqi.keys()], axis=0, ignore_index=True)\n",
    "\n",
    "aqi_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df.iloc[i]['FIRE_YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi['1992']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Niclas/vortrag/lib/python3.6/site-packages/ipykernel_launcher.py:61: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2004\n",
      "2\n",
      "2004\n",
      "3\n",
      "2004\n",
      "4\n",
      "2004\n",
      "5\n",
      "2004\n",
      "6\n",
      "2004\n",
      "7\n",
      "2005\n",
      "8\n",
      "2005\n",
      "9\n",
      "2004\n",
      "10\n",
      "2004\n",
      "11\n",
      "2004\n",
      "12\n",
      "2004\n",
      "13\n",
      "2004\n",
      "14\n",
      "2004\n",
      "15\n",
      "2004\n",
      "16\n",
      "2004\n",
      "17\n",
      "2004\n",
      "18\n",
      "2004\n",
      "19\n",
      "2004\n",
      "20\n",
      "2004\n",
      "21\n",
      "2004\n",
      "22\n",
      "2004\n",
      "23\n",
      "2004\n",
      "24\n",
      "2004\n",
      "25\n",
      "2004\n",
      "26\n",
      "2004\n",
      "27\n",
      "2004\n",
      "28\n",
      "2004\n",
      "29\n",
      "2004\n",
      "30\n",
      "2004\n",
      "31\n",
      "2005\n",
      "32\n",
      "2004\n",
      "33\n",
      "2004\n",
      "34\n",
      "2004\n",
      "35\n",
      "2005\n",
      "36\n",
      "2005\n",
      "37\n",
      "2005\n",
      "38\n",
      "2005\n",
      "39\n",
      "2005\n",
      "40\n",
      "2005\n",
      "41\n",
      "2005\n",
      "42\n",
      "2005\n",
      "43\n",
      "2005\n",
      "44\n",
      "2005\n",
      "45\n",
      "2005\n",
      "46\n",
      "2005\n",
      "47\n",
      "2005\n",
      "48\n",
      "2005\n",
      "49\n",
      "2005\n",
      "50\n",
      "2005\n",
      "51\n",
      "2005\n",
      "52\n",
      "2005\n",
      "53\n",
      "2005\n",
      "54\n",
      "2005\n",
      "55\n",
      "2005\n",
      "56\n",
      "2005\n",
      "57\n",
      "2005\n",
      "58\n",
      "2005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-19a97afd4bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mfitting_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maqiframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mDistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfire_lat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0maqi_lat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfire_lon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0maqi_lon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mDistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDistance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vortrag/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1478\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vortrag/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2089\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2090\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2091\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vortrag/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mis_list_like_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2554\u001b[0m     \u001b[0;31m# allow a list_like, but exclude NamedTuples which can be indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m     return is_list_like(key) and not (isinstance(key, tuple) and\n\u001b[0m\u001b[1;32m   2556\u001b[0m                                       type(key) is not tuple)\n\u001b[1;32m   2557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vortrag/lib/python3.6/site-packages/pandas/core/dtypes/inference.py\u001b[0m in \u001b[0;36mis_list_like\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \"\"\"\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     return (isinstance(obj, Iterable) and\n\u001b[0m\u001b[1;32m    284\u001b[0m             not isinstance(obj, string_and_binary_types))\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vortrag/lib/python3.6/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;31m# Inline the cache checking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#just a sketch\n",
    "#aqi = aqi_all.sort_values(['Date'],axis=0)\n",
    "fire_lat = df['LATITUDE']\n",
    "fire_lon = df['LONGITUDE']\n",
    "dis_date = df['DISCOVERY_DATE']\n",
    "#measure_date = aqi_all['Date']\n",
    "#aqi_lat = aqi_all['Latitude']\n",
    "#aqi_lon = aqi_all['Longitude']\n",
    "new_df = pd.DataFrame()#index=range(df.shape[0]))\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "    a=df.iloc[i]['FIRE_YEAR']\n",
    "    print(a)\n",
    "    aqiframe=aqi[str(a)]\n",
    "    #print(aqiframe)\n",
    "    \n",
    "    measure_date = aqiframe['Date']\n",
    "    aqi_lat = aqiframe['Latitude']\n",
    "    aqi_lon = aqiframe['Longitude']\n",
    "    \n",
    "    MinDistance = 10000000000000\n",
    "    MinDistanceColumn = None\n",
    "\n",
    "    fitting_locations = []\n",
    "    for j in range(aqiframe.shape[0]): \n",
    "        Distance = ((fire_lat.iloc[i]-aqi_lat.iloc[j])**2 + (fire_lon.iloc[i]-aqi_lon.iloc[j])**2)**0.5  \n",
    "        Distance = round(Distance, 6)\n",
    "\n",
    "        if Distance < MinDistance:\n",
    "            MinDistance = Distance\n",
    "            fitting_locations = []\n",
    "            fitting_locations.append(j)\n",
    "            #print(Distance)\n",
    "        elif Distance == MinDistance:\n",
    "            fitting_locations.append(j)\n",
    "            #print(Distance)\n",
    "            \n",
    "    #print(MinDistance)\n",
    "    #print(fitting_locations)\n",
    "\n",
    "    for k in fitting_locations:\n",
    "        if dis_date.iloc[i] < measure_date.astype('datetime64').iloc[k]:\n",
    "            #print(\"k gefunden\")\n",
    "            #print(k)\n",
    "            right_column = aqiframe.iloc[k]\n",
    "            #right_column=right_column.swapaxes(0,1)\n",
    "            #right_column=pd.Series(right_column,index=i)\n",
    "            #print(right_column)\n",
    "            #print(df.iloc[i])\n",
    "            #join together\n",
    "            break\n",
    "    #print(right_column['Date'])\n",
    "    new_series = pd.concat([df.iloc[i],right_column])\n",
    "    #new_series=pd.DataFrame(new_series).transpose()\n",
    "    #print(\"new series:\")\n",
    "    #print(new_series)\n",
    "    #print(\"_________________________\")\n",
    "    new_df=pd.concat([new_df,new_series],axis=1)\n",
    "    #print(\"new df:\")\n",
    "    #print(new_df)\n",
    "    #print(\"_________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AQI CONT_DAY CONT_DAY_OF_WEEK CONT_MONTH                        Category  \\\n",
      "0   68        2                7          2                        Moderate   \n",
      "0   68       12                7          5                        Moderate   \n",
      "0  112       31                1          5  Unhealthy for Sensitive Groups   \n",
      "0   64        3                2          7                        Moderate   \n",
      "0   64        3                2          7                        Moderate   \n",
      "0   97        1                4          7                        Moderate   \n",
      "0   97        2                0          7                        Moderate   \n",
      "0   97        8                5          3                        Moderate   \n",
      "0   97       15                5          3                        Moderate   \n",
      "0   97        2                0          7                        Moderate   \n",
      "0   97        3                2          7                        Moderate   \n",
      "0   97        3                2          7                        Moderate   \n",
      "0   46        3                0          9                            Good   \n",
      "0   47       28                5          9                            Good   \n",
      "0   47        3                3         10                            Good   \n",
      "0   47        3                3         10                            Good   \n",
      "0   67       21                4         10                        Moderate   \n",
      "0   67       17                3         10                        Moderate   \n",
      "0   67       21                3         11                        Moderate   \n",
      "0   33        4                0          6                            Good   \n",
      "0   25       19                2          6                            Good   \n",
      "0   15       22                5          6                            Good   \n",
      "0   13       26                2          6                            Good   \n",
      "0   10        2                0          7                            Good   \n",
      "0   10        6                5          7                            Good   \n",
      "0   23       20                5          7                            Good   \n",
      "0   11       18                4          3                            Good   \n",
      "0   13        5                4          8                            Good   \n",
      "0   11       18                4          3                            Good   \n",
      "0   28       11                5          5                            Good   \n",
      "0   25       19                2          6                            Good   \n",
      "0    9        6                3          2                            Good   \n",
      "0   13       26                2          6                            Good   \n",
      "0    8       30                1          8                            Good   \n",
      "0   13       11                1         10                            Good   \n",
      "0    9        5                2          3                            Good   \n",
      "0   54       11                0          3                        Moderate   \n",
      "0   31       28                0          1                            Good   \n",
      "0   34        6                3          2                            Good   \n",
      "0   34       13                3          2                            Good   \n",
      "0   74       16                2          4                        Moderate   \n",
      "0  112        9                1          5  Unhealthy for Sensitive Groups   \n",
      "0  112        9                1          5  Unhealthy for Sensitive Groups   \n",
      "0   22        5                5          4                            Good   \n",
      "0    6        6                1          6                            Good   \n",
      "0   50       11                0          3                            Good   \n",
      "0   31       11                0          3                            Good   \n",
      "0   49       14                5          6                            Good   \n",
      "0    9       16                7          2                            Good   \n",
      "0   22        2                7          3                            Good   \n",
      "0   13       28                2          5                            Good   \n",
      "0    8       17                5          5                            Good   \n",
      "0   17       27                0          5                            Good   \n",
      "0   19        7                4          4                            Good   \n",
      "0   13       20                1          6                            Good   \n",
      "0   18       26                3          6                            Good   \n",
      "0   27       25                2          6                            Good   \n",
      "0   51        8                5          3                        Moderate   \n",
      "\n",
      "        DISCOVERY_DATE DISCOVERY_DAY DISCOVERY_DAY_OF_WEEK DISCOVERY_MONTH  \\\n",
      "0  2005-02-02 00:00:00             2                     6               2   \n",
      "0  2004-05-12 00:00:00            12                     6               5   \n",
      "0  2004-05-31 00:00:00            31                     1               5   \n",
      "0  2004-06-28 00:00:00            28                     1               6   \n",
      "0  2004-06-28 00:00:00            28                     1               6   \n",
      "0  2004-06-30 00:00:00            30                     6               6   \n",
      "0  2004-07-01 00:00:00             1                     4               7   \n",
      "0  2005-03-08 00:00:00             8                     5               3   \n",
      "0  2005-03-15 00:00:00            15                     5               3   \n",
      "0  2004-07-01 00:00:00             1                     4               7   \n",
      "0  2004-07-02 00:00:00             2                     0               7   \n",
      "0  2004-07-02 00:00:00             2                     0               7   \n",
      "0  2004-09-03 00:00:00             3                     0               9   \n",
      "0  2004-09-28 00:00:00            28                     5               9   \n",
      "0  2004-10-03 00:00:00             3                     3              10   \n",
      "0  2004-10-03 00:00:00             3                     3              10   \n",
      "0  2004-10-06 00:00:00             6                     6              10   \n",
      "0  2004-10-13 00:00:00            13                     6              10   \n",
      "0  2004-11-20 00:00:00            20                     2              11   \n",
      "0  2004-06-04 00:00:00             4                     0               6   \n",
      "0  2004-06-19 00:00:00            19                     2               6   \n",
      "0  2004-06-21 00:00:00            21                     1               6   \n",
      "0  2004-06-25 00:00:00            25                     0               6   \n",
      "0  2004-07-01 00:00:00             1                     4               7   \n",
      "0  2004-07-01 00:00:00             1                     4               7   \n",
      "0  2004-07-20 00:00:00            20                     5               7   \n",
      "0  2004-03-18 00:00:00            18                     4               3   \n",
      "0  2004-08-05 00:00:00             5                     4               8   \n",
      "0  2004-03-18 00:00:00            18                     4               3   \n",
      "0  2004-05-11 00:00:00            11                     5               5   \n",
      "0  2004-06-19 00:00:00            19                     2               6   \n",
      "0  2005-02-05 00:00:00             5                     2               2   \n",
      "0  2004-06-25 00:00:00            25                     0               6   \n",
      "0  2004-08-30 00:00:00            30                     1               8   \n",
      "0  2004-10-11 00:00:00            11                     1              10   \n",
      "0  2005-03-05 00:00:00             5                     2               3   \n",
      "0  2005-03-11 00:00:00            11                     0               3   \n",
      "0  2005-01-27 00:00:00            27                     4               1   \n",
      "0  2005-02-06 00:00:00             6                     3               2   \n",
      "0  2005-02-12 00:00:00            12                     2               2   \n",
      "0  2005-04-16 00:00:00            16                     2               4   \n",
      "0  2005-05-09 00:00:00             9                     1               5   \n",
      "0  2005-05-09 00:00:00             9                     1               5   \n",
      "0  2005-04-05 00:00:00             5                     5               4   \n",
      "0  2005-06-06 00:00:00             6                     1               6   \n",
      "0  2005-03-11 00:00:00            11                     0               3   \n",
      "0  2005-03-11 00:00:00            11                     0               3   \n",
      "0  2005-06-14 00:00:00            14                     5               6   \n",
      "0  2005-02-16 00:00:00            16                     6               2   \n",
      "0  2005-03-02 00:00:00             2                     6               3   \n",
      "0  2005-05-28 00:00:00            28                     2               5   \n",
      "0  2005-05-17 00:00:00            17                     5               5   \n",
      "0  2005-05-27 00:00:00            27                     0               5   \n",
      "0  2005-04-07 00:00:00             7                     4               4   \n",
      "0  2005-06-19 00:00:00            19                     3               6   \n",
      "0  2005-06-25 00:00:00            25                     2               6   \n",
      "0  2005-06-25 00:00:00            25                     2               6   \n",
      "0  2005-03-08 00:00:00             8                     5               3   \n",
      "\n",
      "                  Date Defining Parameter Defining Site FIRE_SIZE_CLASS  \\\n",
      "0  2005-02-03 00:00:00              PM2.5   06-063-1006               0   \n",
      "0  2005-02-03 00:00:00              PM2.5   06-063-1006               0   \n",
      "0  2004-06-03 00:00:00              Ozone   06-061-0004               0   \n",
      "0  2004-06-30 00:00:00              Ozone   06-017-0012               0   \n",
      "0  2004-06-30 00:00:00              Ozone   06-017-0012               0   \n",
      "0  2004-07-07 00:00:00              Ozone   06-017-0012               0   \n",
      "0  2004-07-07 00:00:00              Ozone   06-017-0012               0   \n",
      "0  2004-07-07 00:00:00              Ozone   06-017-0012               1   \n",
      "0  2004-07-07 00:00:00              Ozone   06-017-0012               1   \n",
      "0  2004-07-07 00:00:00              Ozone   06-017-0012               0   \n",
      "0  2004-07-07 00:00:00              Ozone   06-017-0012               0   \n",
      "0  2004-07-07 00:00:00              Ozone   06-017-0012               0   \n",
      "0  2004-09-04 00:00:00              Ozone   06-017-0012               0   \n",
      "0  2004-09-29 00:00:00              Ozone   06-005-0002               1   \n",
      "0  2004-09-29 00:00:00              Ozone   06-005-0002               0   \n",
      "0  2004-09-29 00:00:00              Ozone   06-005-0002               0   \n",
      "0  2004-10-08 00:00:00              Ozone   06-017-0012               6   \n",
      "0  2004-10-08 00:00:00              Ozone   06-017-0012               6   \n",
      "0  2004-10-08 00:00:00              Ozone   06-017-0012               0   \n",
      "0  2004-06-05 00:00:00              PM2.5   35-027-9000               0   \n",
      "0  2004-06-20 00:00:00              PM2.5   35-027-9000               0   \n",
      "0  2004-06-23 00:00:00              PM2.5   35-027-9000               0   \n",
      "0  2004-06-26 00:00:00              PM2.5   35-027-9000               0   \n",
      "0  2004-07-02 00:00:00              PM2.5   35-027-9000               0   \n",
      "0  2004-07-02 00:00:00              PM2.5   35-027-9000               1   \n",
      "0  2004-07-23 00:00:00              PM2.5   35-027-9000               2   \n",
      "0  2004-03-19 00:00:00              PM2.5   35-027-9000               1   \n",
      "0  2004-08-07 00:00:00              PM2.5   35-027-9000               0   \n",
      "0  2004-03-19 00:00:00              PM2.5   35-027-9000               0   \n",
      "0  2004-05-12 00:00:00              PM2.5   35-027-9000               0   \n",
      "0  2004-06-20 00:00:00              PM2.5   35-027-9000               0   \n",
      "0  2005-03-28 00:00:00                 CO   41-017-0002               0   \n",
      "0  2004-06-26 00:00:00              PM2.5   35-027-9000               0   \n",
      "0  2004-08-31 00:00:00              PM2.5   35-027-9000               0   \n",
      "0  2004-10-12 00:00:00              PM2.5   35-027-9000               0   \n",
      "0  2005-03-28 00:00:00                 CO   41-017-0002               0   \n",
      "0  2005-03-26 00:00:00              PM2.5   37-087-0035               1   \n",
      "0  2005-01-28 00:00:00              Ozone   45-073-0001               2   \n",
      "0  2005-04-01 00:00:00              Ozone   37-027-0003               0   \n",
      "0  2005-04-01 00:00:00              Ozone   37-027-0003               3   \n",
      "0  2005-04-17 00:00:00              Ozone   37-011-0002               2   \n",
      "0  2005-05-10 00:00:00              Ozone   37-027-0003               1   \n",
      "0  2005-05-10 00:00:00              Ozone   37-027-0003               2   \n",
      "0  2005-10-19 00:00:00                 CO   41-017-0002               0   \n",
      "0  2005-06-09 00:00:00               PM10   56-031-0805               0   \n",
      "0  2005-03-12 00:00:00              Ozone   45-073-0001               2   \n",
      "0  2005-03-12 00:00:00              PM2.5   41-023-0001               1   \n",
      "0  2005-06-15 00:00:00              Ozone   08-035-0004               1   \n",
      "0  2005-03-28 00:00:00                 CO   41-017-0002               0   \n",
      "0  2005-03-03 00:00:00              PM2.5   41-017-0120               0   \n",
      "0  2005-05-29 00:00:00              PM2.5   41-035-0004               0   \n",
      "0  2005-05-18 00:00:00              PM2.5   41-037-0001               0   \n",
      "0  2005-05-28 00:00:00              PM2.5   41-037-0001               0   \n",
      "0  2005-09-21 00:00:00               PM10   56-033-0003               0   \n",
      "0  2005-06-20 00:00:00              PM2.5   53-013-0001               1   \n",
      "0  2005-06-26 00:00:00              PM2.5   41-037-0001               0   \n",
      "0  2005-06-26 00:00:00              PM2.5   41-035-0004               1   \n",
      "0  2005-03-09 00:00:00               PM10   30-053-0018               1   \n",
      "\n",
      "  FIRE_YEAR LATITUDE LONGITUDE Latitude Longitude STAT_CAUSE_DESCR  \n",
      "0      2005  40.0369  -121.006  39.9371  -120.939                7  \n",
      "0      2004  38.9331  -120.404  39.9371  -120.939                6  \n",
      "0      2004  38.9842  -120.736  39.1002  -120.954                3  \n",
      "0      2004  38.5592  -119.913  38.8116  -120.033                6  \n",
      "0      2004  38.5592  -119.933  38.8116  -120.033                6  \n",
      "0      2004  38.6353  -120.104  38.8116  -120.033                6  \n",
      "0      2004  38.6883  -120.153  38.8116  -120.033                6  \n",
      "0      2005  40.9681  -122.434  38.8116  -120.033                3  \n",
      "0      2005  41.2336  -122.283  38.8116  -120.033                3  \n",
      "0      2004  38.5483  -120.149  38.8116  -120.033                6  \n",
      "0      2004  38.6917   -120.16  38.8116  -120.033                6  \n",
      "0      2004  38.5275  -120.106  38.8116  -120.033                6  \n",
      "0      2004  38.7867  -120.193  38.8116  -120.033                7  \n",
      "0      2004  38.4333   -120.51  38.3399  -120.764                1  \n",
      "0      2004  38.6758   -120.28  38.3399  -120.764                6  \n",
      "0      2004  38.5642  -120.542  38.3399  -120.764                6  \n",
      "0      2004  38.5233  -120.212  38.8116  -120.033                4  \n",
      "0      2004    38.78   -120.26  38.8116  -120.033                4  \n",
      "0      2004   38.945  -120.412  38.8116  -120.033                3  \n",
      "0      2004  33.4408  -105.721  33.4688  -105.535                6  \n",
      "0      2004  33.3072  -105.629  33.4688  -105.535                6  \n",
      "0      2004  33.4444  -105.768  33.4688  -105.535                6  \n",
      "0      2004  33.5594  -105.766  33.4688  -105.535                6  \n",
      "0      2004  33.3081  -105.526  33.4688  -105.535                6  \n",
      "0      2004  33.5453  -105.229  33.4688  -105.535                6  \n",
      "0      2004  33.3158  -105.512  33.4688  -105.535                6  \n",
      "0      2004  33.4444  -105.631  33.4688  -105.535                7  \n",
      "0      2004  33.3681  -105.507  33.4688  -105.535                6  \n",
      "0      2004  33.3853  -105.639  33.4688  -105.535                3  \n",
      "0      2004  33.3828    -105.7  33.4688  -105.535                3  \n",
      "0      2004  33.3797   -105.68  33.4688  -105.535                6  \n",
      "0      2005  43.9956  -121.414  44.0596  -121.325                1  \n",
      "0      2004  33.4967   -105.74  33.4688  -105.535                6  \n",
      "0      2004  33.4178  -105.707  33.4688  -105.535                6  \n",
      "0      2004  33.2553  -105.626  33.4688  -105.535                6  \n",
      "0      2005  44.0433  -121.386  44.0596  -121.325                1  \n",
      "0      2005  35.2283  -82.8844  35.3792  -82.7925                4  \n",
      "0      2005  35.0003  -83.3511  34.8053  -83.2377                0  \n",
      "0      2005  35.9317  -81.7167  35.9358  -81.5303                0  \n",
      "0      2005  36.0017    -81.59  35.9358  -81.5303                3  \n",
      "0      2005   35.985  -81.8517  35.9723  -81.9331                3  \n",
      "0      2005     35.9  -81.6833  35.9358  -81.5303                3  \n",
      "0      2005   36.035   -81.585  35.9358  -81.5303                3  \n",
      "0      2005  43.9556  -121.352  44.0596  -121.325                1  \n",
      "0      2005  42.3547  -105.506  42.1123  -104.868                6  \n",
      "0      2005  35.0014  -83.3842  34.8053  -83.2377                7  \n",
      "0      2005  44.9111  -119.696  44.4172  -118.955                7  \n",
      "0      2005  39.2922  -105.183  39.5345   -105.07                6  \n",
      "0      2005  43.7253  -121.574  44.0596  -121.325                1  \n",
      "0      2005    44.41  -121.316  44.0639  -121.313                3  \n",
      "0      2005  42.1339  -121.234  42.1903  -121.731                6  \n",
      "0      2005  42.3114  -120.903  42.1892  -120.354                7  \n",
      "0      2005  42.9508  -120.836  42.1892  -120.354                0  \n",
      "0      2005  44.8161  -107.331  44.8055  -106.976                7  \n",
      "0      2005  46.2208  -117.785  46.5314  -118.218                6  \n",
      "0      2005  42.3869  -120.809  42.1892  -120.354                6  \n",
      "0      2005  42.5147   -121.12  42.1903  -121.731                6  \n",
      "0      2005  48.9219  -115.093  48.3915  -115.553                3  \n"
     ]
    }
   ],
   "source": [
    "new_df2=new_df.swapaxes(0,1)\n",
    "print(new_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AQI', 'CONT_DAY', 'CONT_DAY_OF_WEEK', 'CONT_MONTH', 'Category',\n",
       "       'DISCOVERY_DATE', 'DISCOVERY_DAY', 'DISCOVERY_DAY_OF_WEEK',\n",
       "       'DISCOVERY_MONTH', 'Date', 'Defining Parameter', 'Defining Site',\n",
       "       'FIRE_SIZE_CLASS', 'FIRE_YEAR', 'LATITUDE', 'LONGITUDE', 'Latitude',\n",
       "       'Longitude', 'STAT_CAUSE_DESCR'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop=new_df2.drop(['DISCOVERY_DATE','Date','LATITUDE','LONGITUDE','Latitude',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Series=pd.DataFrame(new_series)\n",
    "new_Series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Series.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #new_series=pd.concat([df.iloc[i],right_column])\n",
    "    new_df=pd.concat([new_df,new_series])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_column.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBModel()\n",
    "\n",
    "clf.fit(x_train, y_train,\n",
    "        eval_set=[(x_train, y_train), (x_test, y_test)],\n",
    "        eval_metric='logloss',\n",
    "        verbose=True)\n",
    "\n",
    "ypred = clf.predict(x_test)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STAT_CAUSE_DESCR'].value_counts().plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arson = df[df['STAT_CAUSE_DESCR']=='Arson']\n",
    "df_arson['DAY_OF_WEEK'].value_counts().plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(df,size=10):\n",
    "    corr = df.corr()  #the default method is pearson\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.matshow(corr,cmap=plt.cm.Oranges)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)    \n",
    "    plt.show()        \n",
    "plot_corr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot=dfplot.sort_index(by=['FIRE_SIZE'],axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflol=dfplot[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflol.plot(kind='scatter',x='LONGITUDE',y='LATITUDE',color='coral',alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lightning = df#[df['STAT_CAUSE_DESCR']=='Lightning']\n",
    "df_lightning['DISCOVERY_DAY_OF_WEEK'].value_counts().plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DISCOVERY_DAY_OF_WEEK'].value_counts().plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE'].value_counts().head(n=10).plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CA = df[df['STATE']=='CA']\n",
    "df_CA['STAT_CAUSE_DESCR'].value_counts().plot(kind='barh',color='coral',title='causes of fires for CA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
