{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, clear outputs (Cell > All Output > Clear) before committing to Git\n",
    "# There might be a better way\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree, preprocessing\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read wildfire data from SQLITE database\n",
    "cnx = sqlite3.connect('FPA_FOD_20170508.sqlite')\n",
    "df = pd.read_sql_query(\"SELECT FIRE_YEAR,DISCOVERY_TIME,STAT_CAUSE_DESCR,CONT_DATE,CONT_TIME,LATITUDE,LONGITUDE,STATE,DISCOVERY_DATE,FIRE_SIZE,FIRE_SIZE_CLASS FROM 'Fires'\", cnx)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fire discovery and containment dates to yyyy-mm-dd\n",
    "df['DISCOVERY_DATE'] = pd.to_datetime(df['DISCOVERY_DATE'] - pd.Timestamp(0).to_julian_date(), unit='D')\n",
    "df['CONT_DATE'] = pd.to_datetime(df['CONT_DATE'] - pd.Timestamp(0).to_julian_date(), unit='D')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns for month, date, and weekday for discovery and containment dates\n",
    "\n",
    "df['DISCOVERY_MONTH'] = pd.DatetimeIndex(df['DISCOVERY_DATE']).month\n",
    "df['DISCOVERY_DAY'] = pd.DatetimeIndex(df['DISCOVERY_DATE']).day\n",
    "df['DISCOVERY_DAY_OF_WEEK'] = df['DISCOVERY_DATE'].dt.weekday_name\n",
    "\n",
    "#df['CONT_DATE'].fillna(0)\n",
    "df['CONT_MONTH'] = pd.DatetimeIndex(df['CONT_DATE']).month\n",
    "df['CONT_DAY'] = pd.DatetimeIndex(df['CONT_DATE']).day\n",
    "df['CONT_DAY_OF_WEEK'] = df['CONT_DATE'].dt.weekday_name\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "df['STAT_CAUSE_DESCR'] = le.fit_transform(df['STAT_CAUSE_DESCR'])\n",
    "df['STATE'] = le.fit_transform(df['STATE'])\n",
    "df['DISCOVERY_DAY_OF_WEEK'] = le.fit_transform(df['DISCOVERY_DAY_OF_WEEK'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['CONT_DAY_OF_WEEK']=df['CONT_DAY_OF_WEEK'].fillna(\"Unknown\")\n",
    "df['CONT_DAY']=df['CONT_DAY'].fillna(\"0\")\n",
    "df['CONT_MONTH']=df['CONT_MONTH'].fillna(\"0\")\n",
    "df['CONT_TIME']=df['CONT_TIME'].fillna(\"0\")\n",
    "df['DISCOVERY_TIME']=df['DISCOVERY_TIME'].fillna(\"0\")\n",
    "\n",
    "df['CONT_DAY_OF_WEEK'] = le.fit_transform(df['CONT_DAY_OF_WEEK'])\n",
    "df['FIRE_SIZE_CLASS'] = le.fit_transform(df['FIRE_SIZE_CLASS'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CONT_MONTH']=df['CONT_MONTH'].astype('Float64')\n",
    "df['CONT_DAY']=df['CONT_DAY'].astype('Float64')\n",
    "#df['CONT_TIME']=df['CONT_TIME'].astype('Float64')\n",
    "#df['DISCOVERY_DATE']=df['DISCOVERY_DATE'].astype('Float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['CONT_DATE','CONT_TIME','DISCOVERY_TIME','STATE','FIRE_SIZE'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml for severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=df['FIRE_SIZE_CLASS']\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits=df.drop(['FIRE_SIZE','FIRE_SIZE_CLASS','DISCOVERY_DATE','CONT_DATE','STATE','CONT_TIME','DISCOVERY_TIME'],axis=1)\n",
    "logits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(logits,labels,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg.predict(x_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cat=keras.utils.to_categorical(labels,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(logits,labels_cat,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(BatchNormalization(input_shape=[10]))\n",
    "model.add(Dense(500,kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(300,kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(7,kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.01, momentum=0.9,decay=1e-6),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,epochs=10,validation_data=(np.array(x_test),np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Air quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_site_list(site_file):\n",
    "    # Make sure that state, county, and site codes are read as strings\n",
    "    # instead of numbers\n",
    "    col_types = {'State Code': str, 'County Code': str, 'Site Number': str}\n",
    "\n",
    "    sites = pd.read_csv(site_file, dtype=col_types)\n",
    "\n",
    "    # Columns that can be dropped from the site listing\n",
    "    cols_to_drop = [\n",
    "        'Land Use', 'Location Setting', 'Met Site State Code',\n",
    "        'Met Site County Code', 'Met Site Site Number', 'Met Site Type',\n",
    "        'Met Site Distance', 'Met Site Direction', 'GMT Offset',\n",
    "        'Owning Agency', 'Local Site Name', \"Address\", \"Zip Code\",\n",
    "        \"State Name\", \"County Name\", \"City Name\", \"CBSA Name\", \"Tribe Name\",\n",
    "        \"Extraction Date\"\n",
    "    ]\n",
    "\n",
    "    sites = sites.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Convert site closing dates to datetime64 objects (does not work\n",
    "    # with read_csv())\n",
    "    sites['Site Closed Date'] = pd.to_datetime(sites['Site Closed Date'])\n",
    "\n",
    "    # Get sites that have been closed down before 1992 and drop them\n",
    "    # from the list\n",
    "    outdated = sites.loc[\n",
    "        sites['Site Closed Date'] < pd.to_datetime('01-01-1992')]\n",
    "    sites = sites.drop(outdated.index)\n",
    "\n",
    "    # Get the complete site code, add it as a column, and finally\n",
    "    # use the site code as the index\n",
    "    sites['site-code'] = sites['State Code'] + '-' + sites[\n",
    "        'County Code'] + '-' + sites['Site Number']\n",
    "    sites = sites.set_index('site-code')\n",
    "\n",
    "    # Divide the data into two Series: one with latitude coordinates and one\n",
    "    # with longitude coordinates\n",
    "    sites_lat = sites['Latitude']\n",
    "    sites_lon = sites['Longitude']\n",
    "\n",
    "    # Transform the site listings into a dictionary where keys are site\n",
    "    # codes and values are coordinates}\n",
    "    sites_lat = sites_lat.to_dict()\n",
    "    sites_lon = sites_lon.to_dict()\n",
    "\n",
    "    # Return the two dictionaries with the coordinates\n",
    "    return sites_lat, sites_lon\n",
    "\n",
    "\n",
    "def preprocess_aqi(sites_lat, sites_lon, aqi_file):\n",
    "    # Read daily AQI measurements from year XXXX\n",
    "    \n",
    "    # Specify used column names and types to make CSV parsing more efficient\n",
    "    col_types = {'AQI': np.int32, 'Category': str, 'Defining Parameter': str,\n",
    "                 'Defining Site': str}\n",
    "    \n",
    "    col_names = list(col_types.keys()) + ['Date']\n",
    "    \n",
    "    aqi = pd.read_csv(aqi_file, usecols=col_names, dtype=col_types, parse_dates=['Date'])\n",
    "\n",
    "    # Add column containing the site information (including location\n",
    "    # coordinates) to each AQI measurement\n",
    "    aqi['Latitude'] = aqi['Defining Site'].map(sites_lat)\n",
    "    aqi['Longitude'] = aqi['Defining Site'].map(sites_lon)\n",
    "\n",
    "    return aqi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionaries containing latitude and longitude coordinates for AQI measurement sites\n",
    "sites_lat, sites_lon = preprocess_site_list('data/aqi/aqs_sites.csv')\n",
    "\n",
    "# Generate a list of years 1992–2015\n",
    "years = [str(y) for y in range(1992, 2016)]\n",
    "\n",
    "# Generate a dictionary where keys are years (as strings) and values are DataFrames\n",
    "# containing the AQI data with coordinates added\n",
    "aqi = {y: preprocess_aqi(sites_lat, sites_lon, 'data/aqi/daily_aqi_by_county_{}.csv'.format(y)) for y in years}\n",
    "\n",
    "# Combine all AQI data into a single large DataFrame (~7 million rows)\n",
    "aqi_all = pd.concat([aqi[y] for y in aqi.keys()], axis=0, ignore_index=True)\n",
    "\n",
    "#aqi_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative solution for combining AQI data with wildfires:\n",
    "- use the builtin pandas methods for narrowing down the search space when looking for nearest measurements: \n",
    "    - get a daterange from the discovery and containment dates\n",
    "    - further narrow those entries down by including only entries that are within 0.5 degrees of the fire location: this has the problem that 1 degree of longitude varies between ~37 km and 105 km depending on the latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    dist = math.sqrt((lat1 - lat2)**2 + (lon1 - lon2)**2)\n",
    "    return round(dist, 6)\n",
    "\n",
    "\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "def get_aqis_during_fire(year, start_date, end_date):\n",
    "    # Narrow down search by getting measurements from the time of the fire.\n",
    "    # Note that here the date range is defined as DISCOVERY_DATE + 1 to CONT_DATE + 1:\n",
    "    # it seems likely that measurements from the discovery date might have been made before\n",
    "    # the fire was discovered.\n",
    "    # If either of the given dates is a null value, return an empty dataframe\n",
    "    if pd.isnull(start_date) or pd.isnull(end_date):\n",
    "        return pd.DataFrame()\n",
    "    aqi_df = aqi[year].set_index(['Date'])\n",
    "    date_range = pd.date_range(start_date + DateOffset(days=1), end_date + DateOffset(days=1))\n",
    "    return aqi_df.loc[aqi_df.index.isin(date_range)]\n",
    "\n",
    "\n",
    "def get_nearest_measurement(fire_lat, fire_lon, aqi_df):\n",
    "    # Narrow down the search space by only including measurements that are +/- 0.5 degrees \n",
    "    # from the fire coordinates (this translates to a square are with the fire in the\n",
    "    # center: the square is approx. 110 km north-south, and the east-west length varies \n",
    "    # between approx. 37 km and 105 km depending on the latitude)\n",
    "    candidates = aqi_df.loc[((abs(aqi_df['Latitude'] - fire_lat) <= 0.5)\n",
    "                             & (abs(aqi_df['Longitude'] - fire_lon) <= 0.5))]\n",
    "    # If no measurements are within defined range, return NaN\n",
    "    if candidates.empty:\n",
    "        return np.nan\n",
    "    # Loop through the candidate measurements and get the nearest location\n",
    "    nearest = np.argmin([get_distance(fire_lat, fire_lon, candidates.iloc[i]['Latitude'],\n",
    "                                      candidates.iloc[i]['Longitude']) \n",
    "                                      for i in range(candidates.shape[0])])\n",
    "    return candidates.iloc[nearest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# test_df = df.iloc[df.index.isin(range(100))]\n",
    "\n",
    "nearest_measurements = list()\n",
    "rows = df.shape[0]\n",
    "\n",
    "t1 = datetime.now()\n",
    "\n",
    "for row in df.itertuples():\n",
    "    if row.Index % 1000 == 0:\n",
    "        print(\"{}/{}\".format(row.Index, rows))\n",
    "    aqi_candidates = get_aqis_during_fire(str(row.FIRE_YEAR), row.DISCOVERY_DATE, row.CONT_DATE)\n",
    "    if aqi_candidates.empty:\n",
    "        nearest_measurements.append(np.nan)\n",
    "        continue\n",
    "    nearest = get_nearest_measurement(row.LATITUDE, row.LONGITUDE, aqi_candidates)\n",
    "    nearest_measurements.append(nearest)\n",
    "\n",
    "    if row.Index == 9999:\n",
    "        break\n",
    "\n",
    "t2 = datetime.now()\n",
    "\n",
    "delta = t2 - t1\n",
    "\n",
    "print(\"Processing 10,000 entries took {} minutes and {} seconds\".format(\n",
    "                                                    (math.floor(delta.seconds / 60)),\n",
    "                                                    delta.seconds % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fire_lat = df['LATITUDE']\n",
    "fire_lon = df['LONGITUDE']\n",
    "dis_date = df['DISCOVERY_DATE']\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    #print(i)\n",
    "    \n",
    "    a=df.iloc[i]['FIRE_YEAR']    \n",
    "    aqiframe=aqi[str(a)]\n",
    "\n",
    "    \n",
    "    measure_date = aqiframe['Date']\n",
    "    aqi_lat = aqiframe['Latitude']\n",
    "    aqi_lon = aqiframe['Longitude']\n",
    "    \n",
    "    MinDistance = 10000000000000\n",
    "    MinDistanceColumn = None\n",
    "\n",
    "    fitting_locations = []\n",
    "    for j in range(aqiframe.shape[0]): \n",
    "        Distance = ((fire_lat.iloc[i]-aqi_lat.iloc[j])**2 + (fire_lon.iloc[i]-aqi_lon.iloc[j])**2)**0.5  \n",
    "        Distance = round(Distance, 6)\n",
    "\n",
    "        if Distance < MinDistance:\n",
    "            MinDistance = Distance\n",
    "            fitting_locations = []\n",
    "            fitting_locations.append(j)\n",
    "            \n",
    "        elif Distance == MinDistance:\n",
    "            fitting_locations.append(j)\n",
    "\n",
    "    for k in fitting_locations:\n",
    "        if dis_date.iloc[i] < measure_date.astype('datetime64').iloc[k]:\n",
    "            right_column = aqiframe.iloc[k]\n",
    "            break\n",
    "            \n",
    "    new_series = pd.concat([df.iloc[i],right_column])\n",
    "    new_df=pd.concat([new_df,new_series],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2=new_df.swapaxes(0,1)\n",
    "print(new_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop=new_df2.drop(['DISCOVERY_DATE','Date','LATITUDE','LONGITUDE','Latitude','Longitude','STAT_CAUSE_DESCR'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop=drop.drop(['Defining Parameter','Defining Site','Category'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=drop['AQI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits=drop.drop(['AQI'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBModel()\n",
    "\n",
    "clf.fit(x_train, y_train,\n",
    "        eval_set=[(x_train, y_train), (x_test, y_test)],\n",
    "        eval_metric='logloss',\n",
    "        verbose=True)\n",
    "\n",
    "ypred = clf.predict(x_test)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STAT_CAUSE_DESCR'].value_counts().plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arson = df[df['STAT_CAUSE_DESCR']=='Arson']\n",
    "df_arson['DAY_OF_WEEK'].value_counts().plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(df,size=10):\n",
    "    corr = df.corr()  #the default method is pearson\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.matshow(corr,cmap=plt.cm.Oranges)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)    \n",
    "    plt.show()        \n",
    "plot_corr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot=dfplot.sort_index(by=['FIRE_SIZE'],axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflol=dfplot[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflol.plot(kind='scatter',x='LONGITUDE',y='LATITUDE',color='coral',alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lightning = df#[df['STAT_CAUSE_DESCR']=='Lightning']\n",
    "df_lightning['DISCOVERY_DAY_OF_WEEK'].value_counts().plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DISCOVERY_DAY_OF_WEEK'].value_counts().plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE'].value_counts().head(n=10).plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CA = df[df['STATE']=='CA']\n",
    "df_CA['STAT_CAUSE_DESCR'].value_counts().plot(kind='barh',color='coral',title='causes of fires for CA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
