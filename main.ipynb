{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, clear outputs (Cell > All Output > Clear) before committing to Git\n",
    "# There might be a better way\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree, preprocessing\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data for severity prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FIRE_YEAR DISCOVERY_TIME STAT_CAUSE_DESCR  CONT_DATE CONT_TIME   LATITUDE  \\\n",
      "0       2005           1300    Miscellaneous  2453403.5      1730  40.036944   \n",
      "1       2004           0845        Lightning  2453137.5      1530  38.933056   \n",
      "2       2004           1921   Debris Burning  2453156.5      2024  38.984167   \n",
      "3       2004           1600        Lightning  2453189.5      1400  38.559167   \n",
      "4       2004           1600        Lightning  2453189.5      1200  38.559167   \n",
      "\n",
      "    LONGITUDE STATE  DISCOVERY_DATE  FIRE_SIZE FIRE_SIZE_CLASS  \n",
      "0 -121.005833    CA       2453403.5       0.10               A  \n",
      "1 -120.404444    CA       2453137.5       0.25               A  \n",
      "2 -120.735556    CA       2453156.5       0.10               A  \n",
      "3 -119.913333    CA       2453184.5       0.10               A  \n",
      "4 -119.933056    CA       2453184.5       0.10               A  \n"
     ]
    }
   ],
   "source": [
    "# Read wildfire data from SQLITE database\n",
    "cnx = sqlite3.connect('FPA_FOD_20170508.sqlite')\n",
    "df = pd.read_sql_query(\"SELECT FIRE_YEAR,DISCOVERY_TIME,STAT_CAUSE_DESCR,CONT_DATE,CONT_TIME,LATITUDE,LONGITUDE,STATE,DISCOVERY_DATE,FIRE_SIZE,FIRE_SIZE_CLASS FROM 'Fires'\", cnx)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FIRE_YEAR DISCOVERY_TIME STAT_CAUSE_DESCR  CONT_DATE CONT_TIME   LATITUDE  \\\n",
      "0       2005           1300    Miscellaneous 2005-02-02      1730  40.036944   \n",
      "1       2004           0845        Lightning 2004-05-12      1530  38.933056   \n",
      "2       2004           1921   Debris Burning 2004-05-31      2024  38.984167   \n",
      "3       2004           1600        Lightning 2004-07-03      1400  38.559167   \n",
      "4       2004           1600        Lightning 2004-07-03      1200  38.559167   \n",
      "\n",
      "    LONGITUDE STATE DISCOVERY_DATE  FIRE_SIZE FIRE_SIZE_CLASS  \n",
      "0 -121.005833    CA     2005-02-02       0.10               A  \n",
      "1 -120.404444    CA     2004-05-12       0.25               A  \n",
      "2 -120.735556    CA     2004-05-31       0.10               A  \n",
      "3 -119.913333    CA     2004-06-28       0.10               A  \n",
      "4 -119.933056    CA     2004-06-28       0.10               A  \n"
     ]
    }
   ],
   "source": [
    "# Convert fire discovery and containment dates to yyyy-mm-dd\n",
    "df['DISCOVERY_DATE'] = pd.to_datetime(df['DISCOVERY_DATE'] - pd.Timestamp(0).to_julian_date(), unit='D')\n",
    "df['CONT_DATE'] = pd.to_datetime(df['CONT_DATE'] - pd.Timestamp(0).to_julian_date(), unit='D')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FIRE_YEAR DISCOVERY_TIME STAT_CAUSE_DESCR  CONT_DATE CONT_TIME   LATITUDE  \\\n",
      "0       2005           1300    Miscellaneous 2005-02-02      1730  40.036944   \n",
      "1       2004           0845        Lightning 2004-05-12      1530  38.933056   \n",
      "2       2004           1921   Debris Burning 2004-05-31      2024  38.984167   \n",
      "3       2004           1600        Lightning 2004-07-03      1400  38.559167   \n",
      "4       2004           1600        Lightning 2004-07-03      1200  38.559167   \n",
      "\n",
      "    LONGITUDE STATE DISCOVERY_DATE  FIRE_SIZE FIRE_SIZE_CLASS  \\\n",
      "0 -121.005833    CA     2005-02-02       0.10               A   \n",
      "1 -120.404444    CA     2004-05-12       0.25               A   \n",
      "2 -120.735556    CA     2004-05-31       0.10               A   \n",
      "3 -119.913333    CA     2004-06-28       0.10               A   \n",
      "4 -119.933056    CA     2004-06-28       0.10               A   \n",
      "\n",
      "   DISCOVERY_MONTH  DISCOVERY_DAY DISCOVERY_DAY_OF_WEEK  CONT_MONTH  CONT_DAY  \\\n",
      "0                2              2             Wednesday         2.0       2.0   \n",
      "1                5             12             Wednesday         5.0      12.0   \n",
      "2                5             31                Monday         5.0      31.0   \n",
      "3                6             28                Monday         7.0       3.0   \n",
      "4                6             28                Monday         7.0       3.0   \n",
      "\n",
      "  CONT_DAY_OF_WEEK  \n",
      "0        Wednesday  \n",
      "1        Wednesday  \n",
      "2           Monday  \n",
      "3         Saturday  \n",
      "4         Saturday  \n"
     ]
    }
   ],
   "source": [
    "# Add new columns for month, date, and weekday for discovery and containment dates\n",
    "\n",
    "df['DISCOVERY_MONTH'] = pd.DatetimeIndex(df['DISCOVERY_DATE']).month\n",
    "df['DISCOVERY_DAY'] = pd.DatetimeIndex(df['DISCOVERY_DATE']).day\n",
    "df['DISCOVERY_DAY_OF_WEEK'] = df['DISCOVERY_DATE'].dt.weekday_name\n",
    "\n",
    "#df['CONT_DATE'].fillna(0)\n",
    "df['CONT_MONTH'] = pd.DatetimeIndex(df['CONT_DATE']).month\n",
    "df['CONT_DAY'] = pd.DatetimeIndex(df['CONT_DATE']).day\n",
    "df['CONT_DAY_OF_WEEK'] = df['CONT_DATE'].dt.weekday_name\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FIRE_YEAR DISCOVERY_TIME  STAT_CAUSE_DESCR  CONT_DATE CONT_TIME   LATITUDE  \\\n",
      "0       2005           1300                 7 2005-02-02      1730  40.036944   \n",
      "1       2004           0845                 6 2004-05-12      1530  38.933056   \n",
      "2       2004           1921                 3 2004-05-31      2024  38.984167   \n",
      "3       2004           1600                 6 2004-07-03      1400  38.559167   \n",
      "4       2004           1600                 6 2004-07-03      1200  38.559167   \n",
      "\n",
      "    LONGITUDE  STATE DISCOVERY_DATE  FIRE_SIZE FIRE_SIZE_CLASS  \\\n",
      "0 -121.005833      4     2005-02-02       0.10               A   \n",
      "1 -120.404444      4     2004-05-12       0.25               A   \n",
      "2 -120.735556      4     2004-05-31       0.10               A   \n",
      "3 -119.913333      4     2004-06-28       0.10               A   \n",
      "4 -119.933056      4     2004-06-28       0.10               A   \n",
      "\n",
      "   DISCOVERY_MONTH  DISCOVERY_DAY  DISCOVERY_DAY_OF_WEEK  CONT_MONTH  \\\n",
      "0                2              2                      6         2.0   \n",
      "1                5             12                      6         5.0   \n",
      "2                5             31                      1         5.0   \n",
      "3                6             28                      1         7.0   \n",
      "4                6             28                      1         7.0   \n",
      "\n",
      "   CONT_DAY CONT_DAY_OF_WEEK  \n",
      "0       2.0        Wednesday  \n",
      "1      12.0        Wednesday  \n",
      "2      31.0           Monday  \n",
      "3       3.0         Saturday  \n",
      "4       3.0         Saturday  \n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "df['STAT_CAUSE_DESCR'] = le.fit_transform(df['STAT_CAUSE_DESCR'])\n",
    "df['STATE'] = le.fit_transform(df['STATE'])\n",
    "df['DISCOVERY_DAY_OF_WEEK'] = le.fit_transform(df['DISCOVERY_DAY_OF_WEEK'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FIRE_YEAR DISCOVERY_TIME  STAT_CAUSE_DESCR  CONT_DATE CONT_TIME   LATITUDE  \\\n",
      "0       2005           1300                 7 2005-02-02      1730  40.036944   \n",
      "1       2004           0845                 6 2004-05-12      1530  38.933056   \n",
      "2       2004           1921                 3 2004-05-31      2024  38.984167   \n",
      "3       2004           1600                 6 2004-07-03      1400  38.559167   \n",
      "4       2004           1600                 6 2004-07-03      1200  38.559167   \n",
      "\n",
      "    LONGITUDE  STATE DISCOVERY_DATE  FIRE_SIZE  FIRE_SIZE_CLASS  \\\n",
      "0 -121.005833      4     2005-02-02       0.10                0   \n",
      "1 -120.404444      4     2004-05-12       0.25                0   \n",
      "2 -120.735556      4     2004-05-31       0.10                0   \n",
      "3 -119.913333      4     2004-06-28       0.10                0   \n",
      "4 -119.933056      4     2004-06-28       0.10                0   \n",
      "\n",
      "   DISCOVERY_MONTH  DISCOVERY_DAY  DISCOVERY_DAY_OF_WEEK CONT_MONTH CONT_DAY  \\\n",
      "0                2              2                      6          2        2   \n",
      "1                5             12                      6          5       12   \n",
      "2                5             31                      1          5       31   \n",
      "3                6             28                      1          7        3   \n",
      "4                6             28                      1          7        3   \n",
      "\n",
      "   CONT_DAY_OF_WEEK  \n",
      "0                 7  \n",
      "1                 7  \n",
      "2                 1  \n",
      "3                 2  \n",
      "4                 2  \n"
     ]
    }
   ],
   "source": [
    "df['CONT_DAY_OF_WEEK']=df['CONT_DAY_OF_WEEK'].fillna(\"Unknown\")\n",
    "df['CONT_DAY']=df['CONT_DAY'].fillna(\"0\")\n",
    "df['CONT_MONTH']=df['CONT_MONTH'].fillna(\"0\")\n",
    "df['CONT_TIME']=df['CONT_TIME'].fillna(\"0\")\n",
    "df['DISCOVERY_TIME']=df['DISCOVERY_TIME'].fillna(\"0\")\n",
    "\n",
    "df['CONT_DAY_OF_WEEK'] = le.fit_transform(df['CONT_DAY_OF_WEEK'])\n",
    "df['FIRE_SIZE_CLASS'] = le.fit_transform(df['FIRE_SIZE_CLASS'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CONT_MONTH']=df['CONT_MONTH'].astype('Float64')\n",
    "df['CONT_DAY']=df['CONT_DAY'].astype('Float64')\n",
    "#df['CONT_TIME']=df['CONT_TIME'].astype('Float64')\n",
    "#df['DISCOVERY_DATE']=df['DISCOVERY_DATE'].astype('Float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df.drop(['CONT_DATE','CONT_TIME','DISCOVERY_TIME','STATE','FIRE_SIZE'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ml for predicting severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.activations import relu\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: FIRE_SIZE_CLASS, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=df['FIRE_SIZE_CLASS']\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIRE_YEAR</th>\n",
       "      <th>STAT_CAUSE_DESCR</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>DISCOVERY_MONTH</th>\n",
       "      <th>DISCOVERY_DAY</th>\n",
       "      <th>DISCOVERY_DAY_OF_WEEK</th>\n",
       "      <th>CONT_MONTH</th>\n",
       "      <th>CONT_DAY</th>\n",
       "      <th>CONT_DAY_OF_WEEK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005</td>\n",
       "      <td>7</td>\n",
       "      <td>40.036944</td>\n",
       "      <td>-121.005833</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004</td>\n",
       "      <td>6</td>\n",
       "      <td>38.933056</td>\n",
       "      <td>-120.404444</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004</td>\n",
       "      <td>3</td>\n",
       "      <td>38.984167</td>\n",
       "      <td>-120.735556</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004</td>\n",
       "      <td>6</td>\n",
       "      <td>38.559167</td>\n",
       "      <td>-119.913333</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>6</td>\n",
       "      <td>38.559167</td>\n",
       "      <td>-119.933056</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIRE_YEAR  STAT_CAUSE_DESCR   LATITUDE   LONGITUDE  DISCOVERY_MONTH  \\\n",
       "0       2005                 7  40.036944 -121.005833                2   \n",
       "1       2004                 6  38.933056 -120.404444                5   \n",
       "2       2004                 3  38.984167 -120.735556                5   \n",
       "3       2004                 6  38.559167 -119.913333                6   \n",
       "4       2004                 6  38.559167 -119.933056                6   \n",
       "\n",
       "   DISCOVERY_DAY  DISCOVERY_DAY_OF_WEEK  CONT_MONTH  CONT_DAY  \\\n",
       "0              2                      6         2.0       2.0   \n",
       "1             12                      6         5.0      12.0   \n",
       "2             31                      1         5.0      31.0   \n",
       "3             28                      1         7.0       3.0   \n",
       "4             28                      1         7.0       3.0   \n",
       "\n",
       "   CONT_DAY_OF_WEEK  \n",
       "0                 7  \n",
       "1                 7  \n",
       "2                 1  \n",
       "3                 2  \n",
       "4                 2  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits=df.drop(['FIRE_SIZE','FIRE_SIZE_CLASS','DISCOVERY_DATE','CONT_DATE','STATE','CONT_TIME','DISCOVERY_TIME'],axis=1)\n",
    "logits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cat=keras.utils.to_categorical(labels,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(logits,labels_cat,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(BatchNormalization(input_shape=[10]))\n",
    "model.add(Dense(500,kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(300,kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(7,kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.01, momentum=0.9,decay=1e-6),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,epochs=10,validation_data=(np.array(x_test),np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('64%model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='network.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_67 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 1000)              11000     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1200)              1201200   \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_69 (Batc (None, 1200)              4800      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1200)              1441200   \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_70 (Batc (None, 1200)              4800      \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 500)               600500    \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 256)               128256    \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 7)                 1799      \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 3,400,619\n",
      "Trainable params: 3,392,287\n",
      "Non-trainable params: 8,332\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Air quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_site_list(site_file):\n",
    "    # Make sure that state, county, and site codes are read as strings\n",
    "    # instead of numbers\n",
    "    col_types = {'State Code': str, 'County Code': str, 'Site Number': str}\n",
    "\n",
    "    sites = pd.read_csv(site_file, dtype=col_types)\n",
    "\n",
    "    # Columns that can be dropped from the site listing\n",
    "    cols_to_drop = [\n",
    "        'Land Use', 'Location Setting', 'Met Site State Code',\n",
    "        'Met Site County Code', 'Met Site Site Number', 'Met Site Type',\n",
    "        'Met Site Distance', 'Met Site Direction', 'GMT Offset',\n",
    "        'Owning Agency', 'Local Site Name', \"Address\", \"Zip Code\",\n",
    "        \"State Name\", \"County Name\", \"City Name\", \"CBSA Name\", \"Tribe Name\",\n",
    "        \"Extraction Date\"\n",
    "    ]\n",
    "\n",
    "    sites = sites.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Convert site closing dates to datetime64 objects (does not work\n",
    "    # with read_csv())\n",
    "    sites['Site Closed Date'] = pd.to_datetime(sites['Site Closed Date'])\n",
    "\n",
    "    # Get sites that have been closed down before 1992 and drop them\n",
    "    # from the list\n",
    "    outdated = sites.loc[\n",
    "        sites['Site Closed Date'] < pd.to_datetime('01-01-1992')]\n",
    "    sites = sites.drop(outdated.index)\n",
    "\n",
    "    # Get the complete site code, add it as a column, and finally\n",
    "    # use the site code as the index\n",
    "    sites['site-code'] = sites['State Code'] + '-' + sites[\n",
    "        'County Code'] + '-' + sites['Site Number']\n",
    "    sites = sites.set_index('site-code')\n",
    "\n",
    "    # Divide the data into two Series: one with latitude coordinates and one\n",
    "    # with longitude coordinates\n",
    "    sites_lat = sites['Latitude']\n",
    "    sites_lon = sites['Longitude']\n",
    "\n",
    "    # Transform the site listings into a dictionary where keys are site\n",
    "    # codes and values are coordinates}\n",
    "    sites_lat = sites_lat.to_dict()\n",
    "    sites_lon = sites_lon.to_dict()\n",
    "\n",
    "    # Return the two dictionaries with the coordinates\n",
    "    return sites_lat, sites_lon\n",
    "\n",
    "\n",
    "def preprocess_aqi(sites_lat, sites_lon, aqi_file):\n",
    "    # Read daily AQI measurements from year XXXX\n",
    "    \n",
    "    # Specify used column names and types to make CSV parsing more efficient\n",
    "    col_types = {'AQI': np.int32, 'Category': str, 'Defining Parameter': str,\n",
    "                 'Defining Site': str}\n",
    "    \n",
    "    col_names = list(col_types.keys()) + ['Date']\n",
    "    \n",
    "    aqi = pd.read_csv(aqi_file, usecols=col_names, dtype=col_types, parse_dates=['Date'])\n",
    "\n",
    "    # Add column containing the site information (including location\n",
    "    # coordinates) to each AQI measurement\n",
    "    aqi['Latitude'] = aqi['Defining Site'].map(sites_lat)\n",
    "    aqi['Longitude'] = aqi['Defining Site'].map(sites_lon)\n",
    "\n",
    "    return aqi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionaries containing latitude and longitude coordinates for AQI measurement sites\n",
    "sites_lat, sites_lon = preprocess_site_list('data/aqi/aqs_sites.csv')\n",
    "\n",
    "# Generate a list of years 1992–2015\n",
    "years = [str(y) for y in range(1992, 2016)]\n",
    "\n",
    "# Generate a dictionary where keys are years (as strings) and values are DataFrames\n",
    "# containing the AQI data with coordinates added\n",
    "aqi = {y: preprocess_aqi(sites_lat, sites_lon, 'data/aqi/daily_aqi_by_county_{}.csv'.format(y)) for y in years}\n",
    "\n",
    "# Combine all AQI data into a single large DataFrame (~7 million rows)\n",
    "aqi_all = pd.concat([aqi[y] for y in aqi.keys()], axis=0, ignore_index=True)\n",
    "\n",
    "#aqi_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative solution for combining AQI data with wildfires:\n",
    "- use the builtin pandas methods for narrowing down the search space when looking for nearest measurements: \n",
    "    - get a daterange from the discovery and containment dates\n",
    "    - further narrow those entries down by including only entries that are within 0.5 degrees of the fire location: this has the problem that 1 degree of longitude varies between ~37 km and 105 km depending on the latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    dist = math.sqrt((lat1 - lat2)**2 + (lon1 - lon2)**2)\n",
    "    return round(dist, 6)\n",
    "\n",
    "\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "def get_aqis_during_fire(year, start_date, end_date):\n",
    "    # Narrow down search by getting measurements from the time of the fire.\n",
    "    # Note that here the date range is defined as DISCOVERY_DATE + 1 to CONT_DATE + 1:\n",
    "    # it seems likely that measurements from the discovery date might have been made before\n",
    "    # the fire was discovered.\n",
    "    # If either of the given dates is a null value, return an empty dataframe\n",
    "    if pd.isnull(start_date) or pd.isnull(end_date):\n",
    "        return pd.DataFrame()\n",
    "    aqi_df = aqi[year].set_index(['Date'])\n",
    "    date_range = pd.date_range(start_date + DateOffset(days=1), end_date + DateOffset(days=1))\n",
    "    return aqi_df.loc[aqi_df.index.isin(date_range)]\n",
    "\n",
    "\n",
    "def get_nearest_measurement(fire_lat, fire_lon, aqi_df):\n",
    "    # Narrow down the search space by only including measurements that are +/- 0.5 degrees \n",
    "    # from the fire coordinates (this translates to a square are with the fire in the\n",
    "    # center: the square is approx. 110 km north-south, and the east-west length varies \n",
    "    # between approx. 37 km and 105 km depending on the latitude)\n",
    "    candidates = aqi_df.loc[((abs(aqi_df['Latitude'] - fire_lat) <= 0.5)\n",
    "                             & (abs(aqi_df['Longitude'] - fire_lon) <= 0.5))]\n",
    "    # If no measurements are within defined range, return NaN\n",
    "    if candidates.empty:\n",
    "        return np.nan\n",
    "    # Loop through the candidate measurements and get the nearest location\n",
    "    nearest = np.argmin([get_distance(fire_lat, fire_lon, candidates.iloc[i]['Latitude'],\n",
    "                                      candidates.iloc[i]['Longitude']) \n",
    "                                      for i in range(candidates.shape[0])])\n",
    "    return candidates.iloc[nearest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# test_df = df.iloc[df.index.isin(range(100))]\n",
    "\n",
    "nearest_measurements = list()\n",
    "rows = df.shape[0]\n",
    "\n",
    "t1 = datetime.now()\n",
    "\n",
    "for row in df.itertuples():\n",
    "    if row.Index % 1000 == 0:\n",
    "        print(\"{}/{}\".format(row.Index, rows))\n",
    "    aqi_candidates = get_aqis_during_fire(str(row.FIRE_YEAR), row.DISCOVERY_DATE, row.CONT_DATE)\n",
    "    if aqi_candidates.empty:\n",
    "        nearest_measurements.append(np.nan)\n",
    "        continue\n",
    "    nearest = get_nearest_measurement(row.LATITUDE, row.LONGITUDE, aqi_candidates)\n",
    "    nearest_measurements.append(nearest)\n",
    "\n",
    "    if row.Index == 9999:\n",
    "        break\n",
    "\n",
    "t2 = datetime.now()\n",
    "\n",
    "delta = t2 - t1\n",
    "\n",
    "print(\"Processing 10,000 entries took {} minutes and {} seconds\".format(\n",
    "                                                    (math.floor(delta.seconds / 60)),\n",
    "                                                    delta.seconds % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fire_lat = df['LATITUDE']\n",
    "fire_lon = df['LONGITUDE']\n",
    "dis_date = df['DISCOVERY_DATE']\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    #print(i)\n",
    "    \n",
    "    a=df.iloc[i]['FIRE_YEAR']    \n",
    "    aqiframe=aqi[str(a)]\n",
    "\n",
    "    \n",
    "    measure_date = aqiframe['Date']\n",
    "    aqi_lat = aqiframe['Latitude']\n",
    "    aqi_lon = aqiframe['Longitude']\n",
    "    \n",
    "    MinDistance = 10000000000000\n",
    "    MinDistanceColumn = None\n",
    "\n",
    "    fitting_locations = []\n",
    "    for j in range(aqiframe.shape[0]): \n",
    "        Distance = ((fire_lat.iloc[i]-aqi_lat.iloc[j])**2 + (fire_lon.iloc[i]-aqi_lon.iloc[j])**2)**0.5  \n",
    "        Distance = round(Distance, 6)\n",
    "\n",
    "        if Distance < MinDistance:\n",
    "            MinDistance = Distance\n",
    "            fitting_locations = []\n",
    "            fitting_locations.append(j)\n",
    "            \n",
    "        elif Distance == MinDistance:\n",
    "            fitting_locations.append(j)\n",
    "\n",
    "    for k in fitting_locations:\n",
    "        if dis_date.iloc[i] < measure_date.astype('datetime64').iloc[k]:\n",
    "            right_column = aqiframe.iloc[k]\n",
    "            break\n",
    "            \n",
    "    new_series = pd.concat([df.iloc[i],right_column])\n",
    "    new_df=pd.concat([new_df,new_series],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load in the new created data and delete measurement mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/wildfire/wildfires_with_aqi_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['AQI']<500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot of firesize and aqi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Niclas/vortrag/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "aqi=df['AQI'].as_matrix().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Niclas/vortrag/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "fire=df['FIRE_SIZE'].as_matrix().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width = 600, plot_height = 600, \n",
    "           title = 'Example Glyphs',\n",
    "           x_axis_label = 'firesize', y_axis_label = 'aqi',\n",
    "          x_axis_type='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.square(x=fire, y=aqi, size = 1, color = 'black')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(np.array(fire).reshape(-1,1),aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00020529618176290398"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(np.array(fire).reshape(-1,1),aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.002</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.002</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   509.7</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 27 Oct 2018</td> <th>  Prob (F-statistic):</th>  <td>9.17e-113</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:56:01</td>     <th>  Log-Likelihood:    </th> <td>-1.6156e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>289728</td>      <th>  AIC:               </th>  <td>3.231e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>289727</td>      <th>  BIC:               </th>  <td>3.231e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>    0.0010</td> <td> 4.57e-05</td> <td>   22.577</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>102089.827</td> <th>  Durbin-Watson:     </th>  <td>   0.300</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>406148.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td> 1.732</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>        <td> 7.652</td>   <th>  Cond. No.          </th>  <td>    1.00</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.002\n",
       "Model:                            OLS   Adj. R-squared:                  0.002\n",
       "Method:                 Least Squares   F-statistic:                     509.7\n",
       "Date:                Sat, 27 Oct 2018   Prob (F-statistic):          9.17e-113\n",
       "Time:                        13:56:01   Log-Likelihood:            -1.6156e+06\n",
       "No. Observations:              289728   AIC:                         3.231e+06\n",
       "Df Residuals:                  289727   BIC:                         3.231e+06\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0010   4.57e-05     22.577      0.000       0.001       0.001\n",
       "==============================================================================\n",
       "Omnibus:                   102089.827   Durbin-Watson:                   0.300\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           406148.491\n",
       "Skew:                           1.732   Prob(JB):                         0.00\n",
       "Kurtosis:                       7.652   Cond. No.                         1.00\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = fire\n",
    "#X = X[\"FIRE_SIZE\"]\n",
    "y = aqi\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e5f7c5282ef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'STAT_CAUSE_DESCR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'barh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coral'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['STAT_CAUSE_DESCR'].value_counts().plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arson = df[df['STAT_CAUSE_DESCR']=='Arson']\n",
    "df_arson['DAY_OF_WEEK'].value_counts().plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(df,size=10):\n",
    "    corr = df.corr()  #the default method is pearson\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.matshow(corr,cmap=plt.cm.Oranges)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)    \n",
    "    plt.show()        \n",
    "plot_corr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot=dfplot.sort_index(by=['FIRE_SIZE'],axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflol=dfplot[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflol.plot(kind='scatter',x='LONGITUDE',y='LATITUDE',color='coral',alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lightning = df#[df['STAT_CAUSE_DESCR']=='Lightning']\n",
    "df_lightning['DISCOVERY_DAY_OF_WEEK'].value_counts().plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DISCOVERY_DAY_OF_WEEK'].value_counts().plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE'].value_counts().head(n=10).plot(kind='barh',color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CA = df[df['STATE']=='CA']\n",
    "df_CA['STAT_CAUSE_DESCR'].value_counts().plot(kind='barh',color='coral',title='causes of fires for CA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
